{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38ff7622",
   "metadata": {},
   "source": [
    "# generate sample orders.csv (large table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d203477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SampleData/orders.csv', (5000000, 3))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generate dummy data\n",
    "num_rows = 5_000_000  # Approximate size to get ~80MB (depends on data types)\n",
    "np.random.seed(42)\n",
    "\n",
    "data = {\n",
    "    \"order_id\": np.arange(1, num_rows + 1),\n",
    "    \"customer_id\": np.random.randint(100, 200, size=num_rows),\n",
    "    \"amount\": np.random.randint(10, 1000, size=num_rows)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save as CSV\n",
    "file_path = \"SampleData/orders.csv\"\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "file_path, df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b403c5",
   "metadata": {},
   "source": [
    "# generate sample customers.csv (small table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fccdf54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SampleData/customers.csv',\n",
       " (50000, 2),\n",
       "    customer_id          name\n",
       " 0          100  Bob Anderson\n",
       " 1          101  Alice Thomas\n",
       " 2          102  Frank Harris\n",
       " 3          103   Frank Brown\n",
       " 4          104    Jack Smith)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Generate ~50k rows to get around ~1MB CSV\n",
    "num_rows_customers = 50_000\n",
    "\n",
    "# Some meaningful first names and last names\n",
    "first_names = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\", \"Frank\", \"Grace\", \"Helen\", \"Ivy\", \"Jack\"]\n",
    "last_names = [\"Smith\", \"Johnson\", \"Brown\", \"Taylor\", \"Anderson\", \"Thomas\", \"Jackson\", \"White\", \"Harris\", \"Martin\"]\n",
    "\n",
    "# Generate random customer data\n",
    "customer_ids = np.arange(100, 100 + num_rows_customers)\n",
    "names = [f\"{random.choice(first_names)} {random.choice(last_names)}\" for _ in range(num_rows_customers)]\n",
    "\n",
    "customers_df_dummy = pd.DataFrame({\n",
    "    \"customer_id\": customer_ids,\n",
    "    \"name\": names\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "customers_file_path = \"SampleData/customers.csv\"\n",
    "customers_df_dummy.to_csv(customers_file_path, index=False)\n",
    "\n",
    "customers_file_path, customers_df_dummy.shape, customers_df_dummy.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96132de",
   "metadata": {},
   "source": [
    "# PySpark DataFrames without using local data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ce6d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orders DataFrame rows: 5000000\n",
      "+--------+-----------+------+\n",
      "|order_id|customer_id|amount|\n",
      "+--------+-----------+------+\n",
      "|       1|        151|   142|\n",
      "|       2|        192|   165|\n",
      "|       3|        114|   827|\n",
      "|       4|        171|   765|\n",
      "|       5|        160|   211|\n",
      "+--------+-----------+------+\n",
      "only showing top 5 rows\n",
      "Customers DataFrame rows: 50000\n",
      "+-----------+-------------+\n",
      "|customer_id|         name|\n",
      "+-----------+-------------+\n",
      "|        100|  Helen Brown|\n",
      "|        101|Helen Jackson|\n",
      "|        102|  Ivy Jackson|\n",
      "|        103|  Grace White|\n",
      "|        104|  Frank White|\n",
      "+-----------+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import * \n",
    "\n",
    "# Start Spark\n",
    "spark = SparkSession.builder.appName(\"JoinExample\").getOrCreate()\n",
    "\n",
    "# -------------------------------\n",
    "# Create Orders DataFrame (~80MB)\n",
    "# -------------------------------\n",
    "num_rows_orders = 5_000_000\n",
    "np.random.seed(42)\n",
    "\n",
    "orders_data = list(zip(\n",
    "    np.arange(1, num_rows_orders + 1).tolist(),\n",
    "    np.random.randint(100, 200, size=num_rows_orders).tolist(),\n",
    "    np.random.randint(10, 1000, size=num_rows_orders).tolist()\n",
    "))\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "orders_df = spark.createDataFrame(orders_data, schema=orders_schema)\n",
    "print(f\"Orders DataFrame rows: {orders_df.count()}\")\n",
    "orders_df.show(5)\n",
    "\n",
    "# -------------------------------\n",
    "# Create Customers DataFrame (~1MB)\n",
    "# -------------------------------\n",
    "num_rows_customers = 50_000\n",
    "\n",
    "first_names = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\", \"Frank\", \"Grace\", \"Helen\", \"Ivy\", \"Jack\"]\n",
    "last_names = [\"Smith\", \"Johnson\", \"Brown\", \"Taylor\", \"Anderson\", \"Thomas\", \"Jackson\", \"White\", \"Harris\", \"Martin\"]\n",
    "\n",
    "customer_ids = list(range(100, 100 + num_rows_customers))  # plain Python ints\n",
    "names = [f\"{random.choice(first_names)} {random.choice(last_names)}\" for _ in range(num_rows_customers)]\n",
    "\n",
    "customers_data = list(zip(customer_ids, names))\n",
    "\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "customers_df = spark.createDataFrame(customers_data, schema=customers_schema)\n",
    "print(f\"Customers DataFrame rows: {customers_df.count()}\")\n",
    "customers_df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c40587c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle Join execution time: 23.04 seconds\n"
     ]
    }
   ],
   "source": [
    "# Perform a shuffle join\n",
    "import time\n",
    "\n",
    "# Disable auto broadcast join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1) # disable broadcast joins or else spark is smart enough to do it automatically\n",
    "start = time.time()\n",
    "\n",
    "# Trigger Spark action\n",
    "shuffleJoin_df = orders_df.join(customers_df, on=\"customer_id\", how=\"inner\")\n",
    "shuffleJoin_df.count()   # count is better than show() for timing because show() only pulls first 20 rows\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"shuffle Join execution time: {end - start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d1ce92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcast Join execution time: 10.49 seconds\n"
     ]
    }
   ],
   "source": [
    "# Force broadcast join\n",
    "import time\n",
    "start = time.time()\n",
    "broadcast_join_df = orders_df.join(broadcast(customers_df), on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "broadcast_join_df.count()\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Broadcast Join execution time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0385fd5b",
   "metadata": {},
   "source": [
    "# num_rows_orders Vary between (1M->10M)\n",
    "# num_rows_customers (10k â†’ 100k)\n",
    "# Measure time taken for 2 join operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba188de7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ccf40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.functions import broadcast\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Start Spark\n",
    "spark = SparkSession.builder.appName(\"JoinBenchmark\").getOrCreate()\n",
    "\n",
    "def generate_orders(spark, num_rows_orders):\n",
    "    orders_data = list(zip(\n",
    "        np.arange(1, num_rows_orders + 1).tolist(),\n",
    "        np.random.randint(100, 200, size=num_rows_orders).tolist(),\n",
    "        np.random.randint(10, 1000, size=num_rows_orders).tolist()\n",
    "    ))\n",
    "\n",
    "    orders_schema = StructType([\n",
    "        StructField(\"order_id\", IntegerType(), True),\n",
    "        StructField(\"customer_id\", IntegerType(), True),\n",
    "        StructField(\"amount\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    return spark.createDataFrame(orders_data, schema=orders_schema)\n",
    "\n",
    "\n",
    "def generate_customers(spark, num_rows_customers):\n",
    "    first_names = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\", \"Frank\", \"Grace\", \"Helen\", \"Ivy\", \"Jack\"]\n",
    "    last_names = [\"Smith\", \"Johnson\", \"Brown\", \"Taylor\", \"Anderson\", \"Thomas\", \"Jackson\", \"White\", \"Harris\", \"Martin\"]\n",
    "\n",
    "    customer_ids = list(range(100, 100 + num_rows_customers))  # plain Python ints\n",
    "    names = [f\"{random.choice(first_names)} {random.choice(last_names)}\"\n",
    "             for _ in range(num_rows_customers)]\n",
    "\n",
    "    customers_data = list(zip(customer_ids, names))\n",
    "\n",
    "    customers_schema = StructType([\n",
    "        StructField(\"customer_id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    return spark.createDataFrame(customers_data, schema=customers_schema)\n",
    "\n",
    "\n",
    "def measure_time(orders_df, customers_df, join_type=\"shuffle\"):\n",
    "    start = time.time()\n",
    "\n",
    "    if join_type == \"shuffle\":\n",
    "        spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)  # disable broadcast\n",
    "        df = orders_df.join(customers_df, on=\"customer_id\", how=\"inner\")\n",
    "    else:  # broadcast\n",
    "        df = orders_df.join(broadcast(customers_df), on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "    df.count()  # force action\n",
    "    end = time.time()\n",
    "    return round(end - start, 2)\n",
    "\n",
    "\n",
    "# Benchmark parameters\n",
    "orders_sizes = [1_000_000, 5_000_000, 10_000_000]\n",
    "customers_sizes = [10_000, 50_000, 100_000]\n",
    "\n",
    "results = []\n",
    "\n",
    "for o_size in orders_sizes:\n",
    "    for c_size in customers_sizes:\n",
    "        print(f\"\\nTesting orders={o_size:,}, customers={c_size:,}\")\n",
    "\n",
    "        orders_df = generate_orders(spark, o_size)\n",
    "        customers_df = generate_customers(spark, c_size)\n",
    "\n",
    "        shuffle_time = measure_time(orders_df, customers_df, join_type=\"shuffle\")\n",
    "        broadcast_time = measure_time(orders_df, customers_df, join_type=\"broadcast\")\n",
    "\n",
    "        results.append({\n",
    "            \"orders\": o_size,\n",
    "            \"customers\": c_size,\n",
    "            \"shuffle_time\": shuffle_time,\n",
    "            \"broadcast_time\": broadcast_time\n",
    "        })\n",
    "\n",
    "# Convert to Pandas for plotting\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nBenchmark Results:\")\n",
    "print(results_df)\n",
    "\n",
    "# -------------------------------\n",
    "# Plotting\n",
    "# -------------------------------\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "for c_size in customers_sizes:\n",
    "    subset = results_df[results_df[\"customers\"] == c_size]\n",
    "    ax.plot(subset[\"orders\"], subset[\"shuffle_time\"], marker=\"o\", label=f\"Shuffle (cust={c_size})\")\n",
    "    ax.plot(subset[\"orders\"], subset[\"broadcast_time\"], marker=\"s\", linestyle=\"--\", label=f\"Broadcast (cust={c_size})\")\n",
    "\n",
    "ax.set_xlabel(\"Number of Orders\")\n",
    "ax.set_ylabel(\"Execution Time (seconds)\")\n",
    "ax.set_title(\"Shuffle Join vs Broadcast Join Performance\")\n",
    "ax.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03da0368",
   "metadata": {},
   "source": [
    "# result summary:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8144bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# result summary:\n",
    "\n",
    "\n",
    "                                shuffle Join                Broadcast Join\n",
    "Customers ~=1mb Orders~=80mb:   23 seconds                 10.5 seconds\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03599465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aws_spark)",
   "language": "python",
   "name": "aws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
